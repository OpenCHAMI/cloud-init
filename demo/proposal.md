# cloud-init  
Cloud-init supports a myriad of [Data Sources](https://cloudinit.readthedocs.io/en/latest/reference/datasources.html) but so far we have been using [nocloud](https://cloudinit.readthedocs.io/en/latest/reference/datasources/nocloud.html). This allows us to use kernel cmdline parameters to set the endpoints for `cloud-init` to pull it's configs from.  
It's [documented](https://cloudinit.readthedocs.io/en/latest/reference/datasources/nocloud.html#source-files) that the cloud-init client will query a set of urls in a predictable order and then merge the data according to a predictable set of rules:

1. http://endpoint/meta-data
1. http://endpoint/user-data
1. http://endpoint/vendor-data (only if user-data is returned successfully and doesn't contain directives to skip vendor-data)
1. http://endpoint/network-config (only if indicated in user-data or vendor-data)

For merging user-data and vendor-data, cloud-init follows the principle that user-supplied configurations (like user-data) should override vendor defaults (like vendor-data).  For instance, if a root user is defined in both the vendor-data and the user-data, the one in the user-data will be created and the one in vendor-data will be ignored.
 

## background
The idea behind the cloud-init microservice is to provide `user-data`, `vendor-data`, and `meta-data` to a node during post-boot. The contents of the `*-data` payloads should be generated by the cloud-init server based on information provided by the sysadmins as well as information from SMD. 



### v0.0.1
In the most current tagged release [v0.1.1](https://github.com/OpenCHAMI/cloud-init/tree/v0.1.1) the payloads are non-restrictive and have the follow format:
```yaml
name: IDENTIFIER
cloud-init:
  userdata:
  metadata:
  vendordata: 
```
Where the contents of `userdata`, `metadata`, and `vendordata` are dictionaries and no restrictions are made, including whether it is valid cloud-init data or not. You can only add a single `cloud-init` data per `IDENTIFIER`, but there are no restrictions on the number of `IDENTIFIER`s. 

These are stored in memory using the `MemStore` struct
```golang
type MemStore struct {
	list map[string]citypes.CI
}
```
And `citypes` is like so:
```golang 
package citypes

type CI struct {
	Name        string       `json:"name"`
	CIData      CIData     `json:"cloud-init"`
}

type CIData struct {
	UserData map[string]interface{} `json:"userdata"`
	MetaData map[string]interface{} `json:"metadata"`
	VendorData map[string]interface{} `json:"vendordata"`
}
```
When a cloud-init client makes a request to the cloud-init server, the server will attempt to identify the requesting client by looking up it's IP address in the `/EthernetInterfaces` SMD endpoint and use the mapped `ComponentID`. The server will then try to find all the SMD groups this component is a member using `/memberships/{ComponentID}`. 

Once a list of groups is found it will iterate over the groups and check to see if there is an IDENTIFIER that matches the group name.

For every match the `*-data` formats are merge-replaced using [samber/lo](https://github.com/samber/lo), specifically using [Assign](https://github.com/samber/lo?tab=readme-ov-file#assign) which merges two maps like so:
```golang
mergedMaps := lo.Assign(
    map[string]int{"a": 1, "b": 2},
    map[string]int{"b": 3, "c": 4},
)
// map[string]int{"a": 1, "b": 3, "c": 4}
```

This is not ideal because if a `ComponentID` is a member of multiple groups, and those groups share top-level keys (i.e. `write_files`), the returned `*-data` will only include the last group's data, overwriting previous group values. 


### Current Main
In the latest [main](https://github.com/OpenCHAMI/cloud-init) branch of cloud-init, we have made significant changes to how the `*-data` structures are generated. 

First, the payloads for populating the cloud-init server have changed endpoints and structure. Endpoints are now `/cloud-init/groups/{IDENTIFIER}` and the structure changes on whether we are adding `user-data` or `meta-data`.  
`meta-data` looks like so:
```json
{
	"meta-data": {
		"key": "value"
	}
}
```
`user-data`:
```json
{
  "user-data": {
	  "write_files": [
	    {
	      "content": "Hello",
	      "path": "/etc/hello"
	    }
	  ]
	}
}
```
The *only* supported key under `user-data` here is `write_files`, all other keys will be ignored. This not ideal for a lot of reasons and I think we are all in agreement on this. 

The lookup behavior is, I believe, the same as before. The server will lookup the clients IP, get a `ComponentID`, then find all the SMD groups this `ComponentID` is a member of. After this the behavior changes. 

For `meta-data`, and new `meta-data` stucture is built where all the group specific `meta-data` content is bundled into the new stucture with the following format:
```yaml
NID: X
cloud_name: "OpenCHAMI"
#Some more SMD injected data here, still unclear what exactly
groups:
  IDENTIFIER1:
    key: value
  IDENTIFIER2:
    key: value
```
Where `IDENTIFIER1` and `IDENTIFIER2` are hypotheitcal groups a component is a member of AND those groups have stored `meta-data`. For the most part, I think this is a perfectly fine way to merge `meta-data` from multiple groups. 

For `user-data`, a new `user-data` structure is also built but the `user-data` format the cloud-init client expects is more restrictive than `meta-data`. It MUST look like something like
```yaml
#cloud-config
write_files: []
runcmd: []
```
In this case the server makes a new `write_files` list that appends group specific `write_files`. 
For example the payload for `IDENTIFIER1`
```json
{
  "user-data": {
	  "write_files": [
	    {
	      "content": "Hello 1",
	      "path": "/etc/hello1"
	    }
	  ]
	}
}
```
and the payload for `IDENTIDIER2`
```json
{
  "user-data": {
	  "write_files": [
	    {
	      "content": "Hello 2",
	      "path": "/etc/hello2"
	    }
	  ]
	}
}
```
made to `/cloud-init/groups/IDENTIFIER1` and `/cloud-init/groups/IDENTIFIER2` respectively. 

When a component that is a member of `IDENTIFIER1` and `IDENTIFIER2` makes a request to the cloud-init server it will return the following:
```yaml
#cloud-config
write_files:
  - content: "Hello 1"
    path: /etc/hello1
  - content: "Hello 2"
    path: /etc/hello2
```
This is *good* behavior aside from the fact that ONLY `write_files` is supported.

I do not think that `vendor-data` is supported at all in this case. 

### Travis/David Proposal
What I am proposing is that we combine the two ways of building the cloud-init `*-data` payloads.

The v0.1.1 payload structure is the most clear to me. It includes the `IDENTIFIER` in the payload, so if you save these as files it is clear what you are adding data to versus the `/cloud-init/groups/IDENTIFIER` endpoint. It allows to you to add `user-data`, `meta-data`, and `vendor-data` (and eventually `network-config` if we ever figure that out) in a single payload. This has the benefit of being able to support a trivial cloud-init setup where there is no desire to have multiple groups per `ComponentID`. You can have a single cloud-init payload and things will work as expected. 

v0.1.1 is bad at the merge steps. Overwriting data depending on the ordering of the groups is not good behavior and it is also not clear that is what is happening. The current main branch of cloud-init handles this more clearly.  
The `meta-data` has the same issue where conflicting keys will be overwritten by a latter group. 

The behavior of the main branch cloud-init is more robust in the meta-data area. Generating a meta-data that includes injected data from SMD and other generated data is good, and populating a `groups: {}` dictionary with all the meta-data from all groups allows for a lot of flexibility. The `user-data` for `write_files` is something I would like to see happen for all cloud-init `modules`. This leaves the complexity up to the user and makes it so we can support future cloud-init module additions without us having to really do anything.

### Example Use Cases
Managing a really giant file for cloud-init is certainly doable but we should be able to support breaking things up into multiple functions.

For example, assume I have a component that is a member of the following groups:
- ssh
- rsyslog
- chrony
- domain1

SSH, rsyslog, and chrony are intended to be functional things, i.e. actions cloud-init should do on boot. 
I should be able to upload cloud-configs for each of these functions.  
SSH:
```yaml
name: ssh
cloud-init:
  userdata:
    write_files: 
      - content: {{ ds.meta_data.groups.domain1.ssh_root_pub_key }}
        path: /root/.ssh/authorized_keys
```
Rsyslog:
```yaml
name: rsyslog
cloud-init:
  userdata:
    write_files:
      - content: | 
          module(load="impstats" interval="60" severity="7")
          module(load="imuxsock" SysSock.Use="on")
          include(file="/etc/rsyslog.d/*.conf" mode="optional")
          $MainMsgQueueSize 50000
          $MainMsgQueueWorkerThreads 1
          *.info;mail.none;authpriv.none;cron.none                /var/log/messages
          auth.*;authpriv.*;daemon.*                              /var/log/secure
          mail.*                                                  /var/log/maillog
          cron.*                                                  /var/log/cron
          *.emerg                                                 :omusrmsg:*
          uucp,news.crit                                          /var/log/spooler
          local7.*                                                /var/log/boot.log
          action(type="omfwd" target="{{ ds.meta_data.groups.domain1.syslog_aggregator }}" port="514" protocol="tcp")
        path: /etc/rsyslog.conf
    runcmd:
      - systemctl restart rsyslog
```
Chrony:
```yaml
name: chrony
cloud-init:
  userdata:
    write_files: 
      - content: |
          server {{ ds.meta_data.groups.domain1.chrony_server }} iburst
          driftfile /var/lib/chrony/drift
          makestep 1.0 3
          rtcsync
          keyfile /etc/chrony.keys
          leapsectz right/UTC
          logdir /var/log/chrony
        path: /etc/chrony.conf
    runcmd:
      - systemctl restart chronyd
```

The `domain1` group is intended to be a group that stores variables. 
```yaml
name: domain1
cloud-init:
  metadata:
    ssh_root_pub_key: <id_rsa.pub>
    syslog_aggregator: 192.168.0.254
    chrony_server: 192.168.0.125
```

The returned `user-data`, when the client makes a request to `http://cloud-server/cloud-init/user-data` should then look like:
```yaml
#cloud-config
write_files:
  - content: {{ ds.meta_data.groups.domain1.ssh_root_pub_key }}
    path: /root/.ssh/authorized_keys
  - content: |
      module(load="impstats" interval="60" severity="7")
      module(load="imuxsock" SysSock.Use="on")
      include(file="/etc/rsyslog.d/*.conf" mode="optional")
      $MainMsgQueueSize 50000
      $MainMsgQueueWorkerThreads 1
      *.info;mail.none;authpriv.none;cron.none                /var/log/messages
      auth.*;authpriv.*;daemon.*                              /var/log/secure
      mail.*                                                  /var/log/maillog
      cron.*                                                  /var/log/cron
      *.emerg                                                 :omusrmsg:*
      uucp,news.crit                                          /var/log/spooler
      local7.*                                                /var/log/boot.log
      action(type="omfwd" target="{{ ds.meta_data.groups.domain1.syslog_aggregator }}" port="514" protocol="tcp")
    path: /etc/rsyslog.conf
  - content: | 
      server {{ ds.meta_data.groups.domain1.chrony_server }} iburst
      driftfile /var/lib/chrony/drift
      makestep 1.0 3
      rtcsync
      keyfile /etc/chrony.keys
      leapsectz right/UTC
      logdir /var/log/chrony
    path: /etc/chrony.conf
runcmd:
  - systemctl restart rsyslog
  - systemctl restart chronyd
```
and `meta-data` should look like:
```yaml
# SMD injected data
NID: X
# More SMD stuff
groups:
  domain1:
    ssh_root_pub_key: <id_rsa.pub>
    syslog_aggregator: 192.168.0.254
    chrony_server: 192.168.0.125
```
The `meta-data` will show in the instance-data.json under
```json
"ds": {
    "meta_data": {
        "NID": "X",
        "groups": {
            "domain1": {
                "ssh_root_pub_key": "<id_rsa.pub>",
                "syslog_aggregator": "192.168.0.254",
                "chrony_server": "192.168.0.125 "
            }
        }
    }
}
```
Which is how we can use it in the `user-data` templates.

All of this is completely opt-in, so we don't require this to be broken up into pieces. It should be completely possible to combine all of this into a single payload. 

But, by being able to break things up I think we can start thinking of really complex workflows that will allow us to build up nodes in building blocks.

Once scenario I don't have configs for is swapping a node from `slurm` to a `kubernetes` worker. I think we could do this with different groups containing `user-data` with functionality to configure `slurm` versus `kubernetes`, but also retain generic things like `ssh`, `chrony`, `rsyslog`, and lots of other basic things you'd want in any configuration.  

Taking the previous group list
- ssh
- chrony
- rsyslog
- domain1

We *should* be able to add this component to additional slurm groups:
- slurm-client
- slurm-cluster1

where `slurm-client`is something like:
```yaml
name: slurm-client
cloud-init:
  userdata:
    write_files: 
      - content: |
          SLURMD_OPTIONS=--conf-server {{ ds.meta_data.groups.slurm-cluster1.slurmctld_server }}:{{ ds.meta_data.groups.slurm-cluster1.slurmctld_port }}
        path: /etc/sysconfig/slurmd
    runcmd:
      - systemctl restart slurmd
```
and `slurm-cluster1` is some group that defines variables for a slurm cluster called `cluster1`:
```yaml
name: slurm-cluster1
cloud-init:
  metadata:
    slurmctld_server: 192.168.0.250
    slurmctld_port: 6817
```

and the resulting `user-data` and `meta-data` the cloud-init client gets should include the actions and variables it needs to configure `slurmd` during boot. 

If we wanted to move this to a different slurm cluster we could swap it to a `slurm-cluster2` with different settings and the `slurm-client` would still be valid.  
If we wanted to swap this component to be a kubernetes worker node, all we should have to do is move it from the slurm groups to kubernetes groups:
- kube-worker
- kube-cluster1

`kube-worker`:
```yaml
name: kube-worker
cloud-init:
  userdata:
    runcmd: kubeadm join {{ ds.meta_data.groups.kube-cluster1.kube_addr }} --token {{ ds.meta_data.groups.kube-cluster1.kube_join_token }} --discovery-token-ca-cert-hash {{ ds.meta_data.groups.kube-cluster1.kube_cert_hash }}
```
`kube-cluster1`
```yaml
name: kube-cluster1
cloud-init:
  metadata:
    kube_addr: 192.168.5.251:6443
    kube_join_token: blahblahtoken
    kube_cert_hash: blahblahhash
```

These are obviously oversimplifications but if we enable group based cloud-init I think we can accomdate these kinds of workflows, being able to move bare-metal resouces between different types of use-cases like slurm vs kubernetes, while keeping more generic configs in place like ssh access, logging and others.

## Alex Proposal

I propose that we change the internal structure of how our cloud-init data is stored and how sysadmins interact with it.  When defining or updating a group, the sysadmin will upload a complete #cloud-config file along with any necessary key/value pairs for that group.  When the node requests it, the cloud-init server will use the vendor-data file to return urls for each group.  The cloud-init client will follow the list of urls to download and merge the separate "user-data" files and run them.

When assembling the payloads, the meta-data will include all the identity information necessary for the node including things like system name and groups along with any group k/vs that are specified in the group definitions.

### User interaction

Based on this updated struct, the user can specify any file content they want when creating a group and supply any k/v pairs that are needed for this group.
```go
type CloudConfigFile struct {
	Content []byte `json:"content"`
	Name    string `json:"filename"`
}

type GroupData struct {
	Data map[string]string `json:"meta-data,omitempty"`
	File CloudConfigFile   `json:"file,omitempty"`
}
```

As an example, this payload would be used to create a group called `rack5` with a user-data file.

```json
	{
		"name": "x3000",
		"data": {
			"syslog_aggregator": "192.168.0.1"
		},
		"file": {
           "contents": "#template: jinja\n#cloud-config\nrsyslog:\n  remotes: {rack5: {{ vendor_data.groups['x3000'].syslog_aggregator }}}\n  service_reload_command: auto\n",
		}
	}
```

Since this is a fully complete #cloud-config definition with templating enabled, we can reference the variables in meta-data which will be generated with the group information in it.

#### #include

Keeping each user-data separate means that we shift the merge challenge from the server to the client, where it is already supported.  The client already implements merging of multiple full #cloud-config sections.  We cannot concatenate multiple #cloud-config sections in a single file though.  The vendor-data file becomes an #include file which loops through the groups and attempts to download a #cloud-config/user-data file for each one.

```
#template: jinja
#include
{% for group_name in vendor_data.groups.keys() %}
https://{{ vendor_data.cloud_init_base_url }}/{{ group_name }}.yaml
{% endfor %}
```

### Server Implementation

To achieve this, we need to store the file contents for each group in the cloud-init server and be able to deliver them based on a filename like <groupname>.yaml for the nodes that include that group.  The following go structs and their struct tags support this.

```go
type MetaData struct {
	InstanceID    string `json:"instance-id"`
	LocalHostname string `json:"local-hostname"`
	Hostname      string `json:"hostname"`
	ClusterName   string `json:"cluster-name"`
	InstanceData  string `json:"instance-data"`
}

type InstanceData struct {
	V1 struct {
		CloudName        string      `json:"cloud_name"`
		AvailabilityZone string      `json:"availability_zone"`
		InstanceID       string      `json:"instance_id"`
		InstanceType     string      `json:"instance_type"`
		LocalHostname    string      `json:"local_hostname"`
		Region           string      `json:"region"`
		Hostname         string      `json:"hostname"`
		LocalIPv4        interface{} `json:"local_ipv4"`
		CloudProvider    string      `json:"cloud_provider"`
		PublicKeys       []string    `json:"public_keys"`
		VendorData       VendorData  `json:"vendor_data"`
	} `json:"v1"`
}

type VendorData struct {
	Version          string              `json:"version"`
	CloudInitBaseURL string              `json:"cloud_init_base_url"`
	Groups           map[string]Group    `json:"groups"`
	Metadata         []map[string]string `json:"metadata"`
	Attributes       []map[string]string `json:"attributes"`
}

type Group map[string]interface{}
```

### Full example

To illustrate how this will work in our environment, I'll run through a set of commands that the sysadmin can use to customize a set of nodes.  For our example, we will assume that SMD is already populated with three cabinets: x3000,x4000,x5000 and nodes have been sorted into groups for compute, io, and login and into a group for each cabinet.  Every node is part of the "all" group.

Our objectives are:

1. Every node gets the same root password and ssh_authorized_key and SSH configured to allow root login with either a password  or an ssh key.

2. Every node should have a syslog aggregator set based on the cabinet number

3. Compute nodes must have the OpenHPC slurm client installed (we'd normally want to do this in the image.  This is just an example.)

#### Create the all group
First, we'll create our #cloud-config that all nodes get.  It does't need any meta-data.

```bash
#!/bin/bash

# Define the cloud-config content
CLOUD_CONFIG_CONTENT=$(cat <<EOF
#cloud-config
users:
  - name: root
    lock_passwd: false
    passwd: "\$6\$IIUMs.4puvwJ7Yv1\$9H81v9GzvnMmzhdLGFPt0RYpph4oX/cXTsvadE4wXc57IKuLHlN5LhoEf7vkvGAHP7JKKFOJUpmgvLUGMCeKz/"
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDhVqewcj1NwUW7TG9DRY0YnR7BNTovQ2IqiPGgwimdq5s5EU+X2K6JjUpirHayxX4S5KOfp7eFgV3fMo/Wdb+Kq7LJjDEWltLM0xtM8JeCnjVc
a0SKMBraYgqjtmOSipGQ+j7IiwJ+YGIeQA7/rs0o2V5RJThJKYeXgDkHrmwoGumIiGdVeoKAyw2CJS7V6/ISZX8JAsXrIBSfLTcEvQOaiFveOIRAsEhciB1bD4jOFv14iMwiQac36/EncVlxInkFbWznH3G0
+uVapy/fKvbTAOZixLqUjEW5EQqmuOTowB6J/Lro8VV29s5r3wvfPp0dGltIGP3TyJvbWiOetPXZWGnvKfnJcZkJAfoXfIhX1L0y9nROSfGjzaXagIjdXFwvgyXq1MM2+QMjqa5oZdJFC0sWEI55qVmIgJ7I
VtU1rWX/wsGQLdQ9MDL+s+FsOei4JIhRm5aS5JoIBJVVmavd50RfesTYGCmD8qAw07t5UaDs6AexYT0gsJwHc0Xy/Ak=
    shell: /bin/bash

write_files:
  - path: /etc/ssh/sshd_config
    content: |
      PermitRootLogin yes
      PasswordAuthentication yes
      PubkeyAuthentication yes
      AuthorizedKeysFile .ssh/authorized_keys

runcmd:
  - systemctl reload sshd
EOF
)

# Define the JSON payload
JSON_PAYLOAD=$(cat <<EOF
{
  "name": "all",
  "file": {
    "content": "$(echo "$CLOUD_CONFIG_CONTENT" | base64 -w 0)",
  }
}
EOF
)

# Make the POST request
curl -X POST http://cloud-init/groups/ \
     -H "Content-Type: application/json" \
     -d "$JSON_PAYLOAD"

```

#### Create the group information for each cabinet

```bash
curl -X POST http://cloud-init/groups/ \
     -H "Content-Type: application/json" \
     -d 	{
		"name": "x3000",
		"data": {
			"syslog_aggregator": "192.168.0.1"
		},
		"file": {
           "contents": "#template: jinja\n#cloud-config\nrsyslog:\n  remotes: {rack5: {{ vendor_data.groups['x3000'].syslog_aggregator }}}\n  service_reload_command: auto\n",
		}
	}

curl -X POST http://cloud-init/groups/ \
     -H "Content-Type: application/json" \
     -d 	{
		"name": "x4000",
		"data": {
			"syslog_aggregator": "192.168.0.2"
		},
		"file": {
           "contents": "#template: jinja\n#cloud-config\nrsyslog:\n  remotes: {x4000: {{ vendor_data.groups['x4000'].syslog_aggregator }}}\n  service_reload_command: auto\n",
		}
	}

### Both data and file are optional.  This example shows that you can just hardcode everything if you want.
curl -X POST http://cloud-init/groups/ \
     -H "Content-Type: application/json" \
     -d 	{
		"name": "x4000",
		"file": {
           "contents": "#cloud-config\nrsyslog:\n  remotes: {x4000: "192.168.0.5"}\nservice_reload_command: auto\n",
           "encoding": "plain"
		}
	}
```

#### Create the compute group which adds the slurm client from OpenHPC

```bash
#!/bin/bash

# Define the cloud-config content
CLOUD_CONFIG_CONTENT=$(cat <<EOF
#cloud-config
package_update: true
package_upgrade: true
yum_repos:
  OpenHPC:
    baseurl: https://repos.openhpc.community/OpenHPC/3/EL_9/
    enabled: true
    gpgcheck: false
    name: OpenHPC
packages:
  - ohpc-slurm-client
EOF
)

# Define the JSON payload
JSON_PAYLOAD=$(cat <<EOF
{
  "name": "compute",
  "file": {
    "content": "$(echo "$CLOUD_CONFIG_CONTENT" | base64 -w 0)",
    "filename": "cloud-config.yaml",
    "encoding": "base64"
  }
}
EOF
)

# Make the POST request
curl -X POST http://cloud-init/groups/ \
     -H "Content-Type: application/json" \
     -d "$JSON_PAYLOAD"

```

### Cloud-Init Client Behavior for x4000c4b0n1

To simplify this, I will explain it without our current token-download system and then I will explain the slight modification.

Upon boot, the client will first request `/meta-data` which will return the payload below.  Notice that things like instance-id and hostname are automatically generated based on information from SMD.  The vendor data also has the nid, role and cabinet from SMD for use in templating.

```yaml
instance-id: i-DEADBEEF
local-hostname: ve0007
hostname: ve0007.venado.int.lanl.gov
v1:
  cloud_name: "OpenCHAMI"
  availability_zone: "us-west-1"
  region: "us-west"
  system_name: "venado"
  cloud_provider: "lanl"
  vendor_data:
    version: 1.0
    nid: 7
    role: compute
    cabinet: x4000
    cloud_init_base_url: "http://192.168.13.3:8080"
    groups:
      all:
        description: "All nodes in venado get this group"
      compute:
        description: "Compute group"
      x4000:
        description: "Cabinet X4000 group"
        syslog_aggregator: "10.10.0.1"
```

Once that is complete, it will retrieve the `/user-data` url which we will want to allow users to submit individually in the future if they desire so we'll just send a placeholder that is effectively a no-op.

```
#cloud-config
```

Once that is complete, it will retrieve the vendor data which is a list of the other files to download.

```
#template: jinja
#include
{% for group_name in vendor_data.groups.keys() %}
https://{{ vendor_data.cloud_init_base_url }}/{{ group_name }}.yaml
{% endfor %}
```

This should expand to:
```
#template: jinja
#include
http://192.168.13.3:8080/all.yaml
http://192.168.13.3:8080/compute.yaml
http://192.168.13.3:8080/x4000.yaml
```

The cloud-init client will then download each one of these in sequence and merge them on the node.